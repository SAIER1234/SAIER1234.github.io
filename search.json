[{"title":"0.课程简介","url":"/%E6%95%B0%E6%8D%AE%E5%BA%93%E7%B3%BB%E7%BB%9F/CMU-15-445/%E6%95%B0%E6%8D%AE%E5%BA%93%E7%B3%BB%E7%BB%9F/CMU-15-445/0-%E8%AF%BE%E7%A8%8B%E7%AE%80%E4%BB%8B/","content":"1.课程链接&lt;网站&gt;(https://15445.courses.cs.cmu.edu/spring2023/schedule.html)\n","categories":["数据库系统","CMU-15-445"],"tags":["CS学习"]},{"title":"0.课程简介","url":"/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/UMich-eecs498/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/UMich-EECS498/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-UMich-EECS498-0-%E8%AF%BE%E7%A8%8B%E7%AE%80%E4%BB%8B/","content":"1.课程链接因为听说好上手，而且能够更加熟练使用pycharm,具体咋样我上上看再评价。 &lt;网站&gt;(https://web.eecs.umich.edu/~justincj/teaching/eecs498/WI2022/)\n&lt;课程视频&gt;(https://www.youtube.com/playlist?list=PL5-TkQAfAZFbzxjBHtzdVCWE0Zbhomg7r)\n","categories":["机器学习","UMich-eecs498"],"tags":["CS学习"]},{"title":"0.课程简介","url":"/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/UCB-188/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/UCB-188/0-%E8%AF%BE%E7%A8%8B%E7%AE%80%E4%BB%8B/","content":"1. 课程链接还没学，网站先放在这：&lt;网站&gt;(https://inst.eecs.berkeley.edu/~cs188/sp24/)\n","categories":["人工智能","UCB-188"],"tags":["CS学习"]},{"title":"0.shell介绍","url":"/shell/MIT-Missing-Semester/shell/MIT-Missing-Semester/0.shell%E4%BB%8B%E7%BB%8D/","content":"1.为什么要学习shell?节省时间，更高效解决问题。\n2.学习的课程MIT-Missing-Semester,网站是：(https://missing-semester-cn.github.io/2020/).\n","categories":["shell","MIT-Missing-Semester"],"tags":["CS学习"]},{"title":"1.shell初认识","url":"/shell/MIT-Missing-Semester/shell/MIT-Missing-Semester/1-shell%E5%88%9D%E8%AE%A4%E8%AF%86/","content":"1.shell是什么？如今的计算机有着多种多样的交互接口让我们可以进行指令的输入，从炫酷的图像用户界面（GUI），语音输入甚至是 AR&#x2F;VR 都已经无处不在。 这些交互接口可以覆盖 80% 的使用场景，但是它们也从根本上限制了您的操作方式——你不能点击一个不存在的按钮或者是用语音输入一个还没有被录入的指令。 为了充分利用计算机的能力，我们不得不回到最根本的方式，使用文字接口：Shell\n2.使用shell\n\n25435@saier MINGW64是主机名字\n~是位置home\n$ 表非root\n\n2.1简单命令\ndate :输出日期\necho xxx : 打印注意shell的空格，参数要有空格可单引号或\\转移符号\n\n2.1 shell寻路原理1.执行echo,先去$PATH找，找到就执行。2.也可以用which,找到路径，然后直接用路径执行。\n2.2 导航\n用&#x2F;分割，windows则是\\ .\n若某路径以&#x2F;开头：绝对路径，否则相对路径\npwd: 获取当前工作目录。\ncd : . :当前目录  ..:上级目录\nls: 查看目录底下的文件\n介绍标记和选项： 他们以 -开头 ，执行程序可用-h or--help 打印帮助信息。\nd表示这是Users的一个目录。接下来9字符3个一组，分布代表文件所有者(Users),用户组，其他所有人，只有文件所有者可以改（有w）,其他只能读r  与x执行。\nmv: 重命名或移动文件\ncp: 拷贝文件\nmkdir : 新建文件夹\n使用man + 程序名，会展现用户手册，按q可退出\ncat : 用于读取，创建，合并，和显示文件内容创建： cat &gt; 文件名 之后按ctrl +d退出\n\n2.3 程序间创建链接正常来说键盘为输入，显示器为输出，但可重定向输入输出流。\n\n使用&lt;和&gt;\n\n注意：&lt;是覆盖重定向，&lt;&lt;是追加重定向\n\n使用管道（pipes） 与|:将前一个程序输出和后一个程序输入连接\n\n2.4 强大工具sudo\n我们一般不以根用户登录，root users权限最强，怕搞破坏，所以需要时可以使用sudo命令。（super user do） tee : 规范标准输入输出流 echo 3 | sudo tee brightness :正确。 sudo echo 3 | brightness:错误，每个符号都是通过shell执行而不是程序单独执行，brightness打不开，无权限,而且brightness是文件而不是程序，前面要加上tee .\n 使用.\\路径 执行某文件。\n chmod: \n","categories":["shell","MIT-Missing-Semester"],"tags":["CS学习"]},{"title":"0.深入探究LLM","url":"/Vibe-Coding/CS146S/%E7%AC%AC%E4%B8%80%E5%91%A8/Vibe-Coding/CS146S/%E7%AC%AC%E4%B8%80%E5%91%A8-%E6%B7%B1%E5%85%A5%E6%8E%A2%E7%A9%B6LLM/","content":" （都是notebooklm生成的，原视频3个多小时实在顶不住）\n基础概念入门地图：跟随安德烈·卡帕西探索大语言模型（LLM）作为一名人工智能教育专家，我见证了无数学习者在海量的信息中迷失。如果你想真正触及大语言模型（LLM）的灵魂，安德烈·卡帕西（Andrej Karpathy）的课程体系是公认的“金标准”。本指南将基于其核心教学理念，为你拆解 LLM 的底层逻辑，构建一套严密的知识坐标系。1. 走近大语言模型：AI 的“大脑”是如何思考的？在卡帕西的教学框架中，大语言模型（LLM）剥离神秘感后，其本质极其纯粹：它是一个下一个 Token 预测器（Next-Token Predictor）。它并不具备人类意义上的“思考”，而是模型在预训练阶段，通过海量文本学习概率分布，从而掌握了模仿人类表达的能力。当你输入一段话，模型实际上是在庞大的参数空间中进行统计推理，计算并输出序列中下一个最可能出现的“词元”。一句话定义 LLM： 大语言模型是一个基于 Transformer 架构的统计推理引擎，其核心任务是根据上下文语境，预测并生成概率分布最高的后续词元（Token）。然而，在模型能够开始这种复杂的“概率预测”之前，它面临一个基础工程问题：作为一套数学模型，它是如何处理我们输入的人类文字的？2. 分词器（Tokenizer）：文字与数字之间的桥梁计算机无法直接理解字符。在卡帕西的《Let’s build the GPT Tokenizer》中，他深入演示了文本如何转化为数字序列。分词器（Tokenizer）Token（词元）。注意，词元通常不是单词或单字，而是“子词单位”（Sub-words），这通过 UTF-8 编码和诸如 tiktoken 库实现的字节对编码（BPE）算法来完成。正如我们在教学中强调的，词表的大小（例如 GPT-2 的 50,257 个词元）决定了模型的“词汇量”。阶段表示形式示例&#x2F;技术细节原始文本自然语言字符串“AI Education”分词处理子词（Sub-word）序列[“AI”, “ Education”]数字转化整数 ID 序列 (Tokens)[15592, 11634]（基于 tiktoken 词表）当文字成功转化为这种结构化的数字 ID 序列后，它们将进入 LLM 最核心的加工厂——Transformer。3. Transformer 架构：LLM 的动力引擎Transformer 是 LLM 的心脏。卡帕西在“从零开始复现 GPT”的系列教程中，将 Transformer 描述为一种处理 Token 之间关系的精密网络。其核心逻辑在于注意力机制（Attention Mechanism）——这可以被理解为 Token 之间的“交流阶段”，每一个词元都会“观察”其他词元，以交换信息并确定自己在当前语境下的含义。Transformer 的 3 个核心教育维度：• 并行处理能力： 摆脱了传统 RNN 的序列依赖，允许模型在大规模集群上高效训练，这是现代大模型“规模效应”的基础。• 长距离依赖： 确保模型在处理长文档时，结尾处的 Token 依然能精准“定位”开头提到的主语。• 注意力分配（Attention）： 模型会动态计算权重。对于学习者而言，这意味着模型学会了“抓重点”，理解了词与词之间的逻辑强弱。架构搭建完毕后，模型仍是一个“空壳”，它需要经历不同的成长阶段来获取知识与智慧。4. 模型成长的双重奏：预训练（Pre-training）与微调（Fine-tuning）卡帕西在其《Intro to LLMs》中清晰地界定了模型成长的两个关键阶段。预训练是“炼金”，而微调是“雕琢”。维度预训练 (Pre-training)微调 (Fine-tuning)核心目标互联网文档补全（学习世界百科知识）对话意图对齐（学习成为有用助手）数据来源数万亿词元的公开互联网文本（垃圾与精华并存）数千至数万条高质量人类对话、指令样本计算消耗极大（通常需数千 GPU 运行数月，如 GPT-2 124M 级）极小（在已有权重上进行小规模梯度更新）预训练让模型成为了一个什么都读过的“博学家”，而微调则赋予了它作为“助手”的职业道德和交流规范。经过这两阶段的洗礼，模型完成了一次本质进化：它从简单的“文字接龙手”变成了能够理解并执行复杂指令的智能体。5. 核心演进：从预测下一个 Token 到理解复杂指令这是大语言模型通往 AGI（通用人工智能）的关键转折。卡帕西指出，通过**指令微调（Instruction Tuning）**和 RLHF（基于人类反馈的强化学习），我们能够塑造模型的“价值观”和回复倾向，使其从单纯的概率预测跨越到意图理解。为什么这种进化对现代 AI 应用至关重要？\n\n行为对齐（Alignment）： 确保模型不会在被要求写代码时，反而去预测一段关于代码讨论的无意义闲聊。\n安全性与风格控制： 通过 RLHF，我们可以降低模型产生有害内容的概率，并调节其对话的“语气”（即 Vibe）。\n从预测到自动化： 正如姚顺雨（Shunyu Yao）与卡帕西探讨的，模型正在从“Next-token”转向“数字自动化”，具备了调用工具和解决多步问题的逻辑链条。这种从“概率”到“能力”的跃迁，正是你开启 AI 探索之旅的最佳切入点。\n\n","categories":["Vibe-Coding","CS146S","第一周"],"tags":["CS学习"]},{"title":"0.课程简介","url":"/%E7%A6%BB%E6%95%A3%E6%95%B0%E5%AD%A6/CS70/%E7%A6%BB%E6%95%A3%E6%95%B0%E5%AD%A6/CS70/0-%E8%AF%BE%E7%A8%8B%E7%AE%80%E4%BB%8B/","content":"1.课程链接我自己懒的看视频了，所以直接把别人的整理好的hw和note等搬上来：&lt;网站&gt;(https://github.com/PKUFlyingPig/UCB-CS70/fork)\n我自己是fork下来然后搞了个PDF翻译note,一直看英语太牢了，本来就难再搞个英语难上加难 \n","categories":["离散数学","CS70"],"tags":["CS学习"]},{"title":"0.课程简介","url":"/Vibe-Coding/CS146S/Vibe-Coding/CS146S/0-%E8%AF%BE%E7%A8%8B%E7%AE%80%E4%BB%8B/","content":"1.课程链接(https://github.com/ShouZhengAI/CS146S_CN)\n","categories":["Vibe-Coding","CS146S"],"tags":["CS学习"]}]